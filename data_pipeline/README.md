Airflow Sentiment Analysis Pipeline

This project sets up an Apache Airflow data pipeline using Docker Compose to manage sentiment analysis tasks. The pipeline processes and analyzes review data, storing outputs in CSV files.

Project Structure

DAGs: Defined in data_pipeline.py within the dags folder.
Data Storage: Processed CSV files are saved in the data directory.
Tests: Unit tests for the pipeline are located in the tests folder.

Setup and Installation

Prerequisites
Docker and Docker Compose should be installed on your system.

Getting Started
Clone the Repository:

git clone https://github.com/madhurima-vanga/Amazon-Customer-Sentiment-Analyser.git
cd Amazon-Customer-Sentiment-Analyser/data_pipeline

Initialize Airflow:
Before running Airflow, initialize the database and create necessary directories:

docker compose up airflow-init

Start Airflow Services:
Run the following command to start the Airflow scheduler, web server, and other services:

docker compose up

This will start all necessary containers, including the Airflow web server, which can be accessed at http://localhost:8080.

Accessing the Airflow Web Interface
Once the services are up, visit http://localhost:8080 to access the Airflow web interface. Use the default login credentials:

Username: airflow
Password: airflow

Running Tests

Unit tests for the pipeline are written with pytest and located in the tests folder.

Install Dependencies (if not done in a virtual environment):

pip install pytest

Run all tests with:


pytest tests/

This will execute all unit tests and display results, verifying the functionality of each pipeline component.

Data Storage

Processed data from the Airflow tasks, such as CSV files generated by the pipeline, are stored in the data directory within the project. This folder will contain:

reviews.csv: The raw reviews dataset.
metadata.csv: The raw metadata dataset.
merged_data.csv: Merged review and metadata file.
Additional CSV files created during data preprocessing and analysis.

Cleaning Up

To stop the containers, press CTRL+C in the terminal where Docker Compose is running.

To remove all containers and reset Airflow:


docker compose down --volumes --remove-orphans

This command will stop all services and remove volumes, clearing the Airflow database and any temporary files.